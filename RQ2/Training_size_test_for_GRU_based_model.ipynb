{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training size test for GRU-based model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This jupyter nodebook is for RQ2 of the project to measure what is the minimum required data of SO dataset to achieve acceptable results compared to maximum data size. I start from 10% of the total amount of data to 100%."
      ],
      "metadata": {
        "id": "d_O4sGPh8JUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "Rbgo8IbD8KpU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSsLMO_FBhiP",
        "outputId": "ad81729a-896b-4066-ea45-0e4653179003"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBr5eUCxB_PZ"
      },
      "source": [
        "!unzip /content/drive/MyDrive/SE\\ project/dataset/EMTK_datasets-v1.0.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArWqQX1MCX2S"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('/content/collab-uniba-EMTK_datasets-c200f78/so/emotions/Emotions_GoldSandard_andAnnotation.xlsx',sheet_name=None)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1mMC9KHDvBI",
        "outputId": "01869b9c-1209-41a7-ebb6-4380768eb110"
      },
      "source": [
        "df"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Anger_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Fear_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Joy_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        JOY        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Love_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...       LOVE        NaN        NaN\n",
              " 1        A  Second           2  ...       LOVE        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...       LOVE        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...       LOVE        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Sadness_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...    SADNESS        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Surprise_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns]}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mioJoc2uDMTH",
        "outputId": "38bc0574-92f6-419c-a552-4c7628812a77"
      },
      "source": [
        "df[\"Love_all\"]"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e2a37cbe-2087-4bdc-905f-5e86b75c4fe0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Group</th>\n",
              "      <th>Set</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Text</th>\n",
              "      <th>rater 1</th>\n",
              "      <th>rater 2</th>\n",
              "      <th>rater 3</th>\n",
              "      <th>Gold Label</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>1</td>\n",
              "      <td>SVG transform on text attribute works excellen...</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "      <td>X</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>2</td>\n",
              "      <td>Excellent! This is exactly what I needed. Thanks!</td>\n",
              "      <td>X</td>\n",
              "      <td>x</td>\n",
              "      <td>X</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>3</td>\n",
              "      <td>Have added a modern solution as of May 2014 in...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>4</td>\n",
              "      <td>Have you tried removing 'preload' attribute? (...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>5</td>\n",
              "      <td>A smarter, entirely C++-way of doing what you ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>496</td>\n",
              "      <td>Yes - that feature is extremely useful for wri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>497</td>\n",
              "      <td>Works great! And you can add \"desc\" after the ...</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>498</td>\n",
              "      <td>Yeah, I didn't know about the non-greedy thing...</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>499</td>\n",
              "      <td>Fortunately I'm doing *very* little with Offic...</td>\n",
              "      <td>x</td>\n",
              "      <td>X</td>\n",
              "      <td>x</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>500</td>\n",
              "      <td>Another very fast approach is the [seek method...</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2a37cbe-2087-4bdc-905f-5e86b75c4fe0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2a37cbe-2087-4bdc-905f-5e86b75c4fe0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2a37cbe-2087-4bdc-905f-5e86b75c4fe0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              "0        A  Second           1  ...       LOVE        NaN        NaN\n",
              "1        A  Second           2  ...       LOVE        NaN        NaN\n",
              "2        A  Second           3  ...        NaN        NaN        NaN\n",
              "3        A  Second           4  ...        NaN        NaN        NaN\n",
              "4        A  Second           5  ...        NaN        NaN        NaN\n",
              "...    ...     ...         ...  ...        ...        ...        ...\n",
              "4795     D   Third         496  ...        NaN        NaN        NaN\n",
              "4796     D   Third         497  ...       LOVE        NaN        NaN\n",
              "4797     D   Third         498  ...        NaN        NaN        NaN\n",
              "4798     D   Third         499  ...       LOVE        NaN        NaN\n",
              "4799     D   Third         500  ...        NaN        NaN        NaN\n",
              "\n",
              "[4800 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxq2ahs_D55P"
      },
      "source": [
        "text = []\n",
        "neutral_emotional = []\n",
        "label_sets = []\n",
        "\n",
        "for i in range(4800):\n",
        "  \n",
        "  label = 0\n",
        "  label_set = []\n",
        "  for sheet in ['Love_all', 'Joy_all', 'Anger_all', 'Sadness_all', 'Fear_all', 'Surprise_all']:\n",
        "    gold_label = df[sheet].loc[i].iat[7]\n",
        "    if gold_label in ['LOVE', 'JOY', 'ANGER', 'SADNESS', 'FEAR', 'SURPRISE']:\n",
        "      label = 1\n",
        "      label_set.append(gold_label)\n",
        "  if not 'SURPRISE' in label_set:\n",
        "    neutral_emotional.append(label)\n",
        "    label_sets.append(label_set)\n",
        "    text.append(df['Love_all'].loc[i].iat[3])\n",
        "  "
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiSCS3Mxg0zA"
      },
      "source": [
        "def not_both(label_set,i):\n",
        "  positive = ['LOVE','JOY']\n",
        "  negative = ['SADNESS', 'FEAR', 'ANGER']\n",
        "  is_positive = False\n",
        "  is_negative = False\n",
        "  for label in label_set:\n",
        "    if label in positive:\n",
        "      is_positive = True\n",
        "    elif label in negative:\n",
        "      is_negative = True\n",
        "  if is_negative and is_positive:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def find_semantic(label_set):\n",
        "  positive = ['LOVE','JOY']\n",
        "  negative = ['SADNESS', 'FEAR', 'ANGER']\n",
        "  if label_set[0] in positive:\n",
        "    return 1\n",
        "  return 0"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVfodv6Dcm8W"
      },
      "source": [
        "text_semantic = []\n",
        "semantic = []\n",
        "\n",
        "for i in range(4755):\n",
        "  if neutral_emotional[i]:\n",
        "    if not_both(label_sets[i],i):\n",
        "      text_semantic.append(text[i])\n",
        "      semantic.append(find_semantic(label_sets[i]))\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcUWip-WlGFt"
      },
      "source": [
        "text_all = []\n",
        "three_cat = []\n",
        "\n",
        "for i in range(4755):\n",
        "  if neutral_emotional[i]==0:\n",
        "    text_all.append(text[i])\n",
        "    three_cat.append('Neutral')\n",
        "for i in range(len(text_semantic)):\n",
        "  text_all.append(text_semantic[i])\n",
        "  if semantic[i]:\n",
        "    three_cat.append('Positive')\n",
        "  else:\n",
        "    three_cat.append('Negative')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muMmkfO74RTr",
        "outputId": "af662ebc-e888-450a-a21c-07c5baa644ba"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4730"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThoTEPk7xoK3"
      },
      "source": [
        "# Model setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvPs_TW45uhC"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyGXUB1d4iTS",
        "outputId": "be2b17a1-ed73-4bde-a13c-d1c0079c22eb"
      },
      "source": [
        "# define example\n",
        "data = three_cat\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Neutral' 'Neutral' 'Neutral' ... 'Positive' 'Positive' 'Positive']\n",
            "[1 1 1 ... 2 2 2]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3eLp3pM3fHL",
        "outputId": "417f7cd0-09c1-4606-9487-686d6d9b283e"
      },
      "source": [
        "len(text_all)*0.7"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3311.0"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_GRU  = []"
      ],
      "metadata": {
        "id": "5JsxpVD_FpWJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 %"
      ],
      "metadata": {
        "id": "4VOchltPC3pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.1"
      ],
      "metadata": {
        "id": "UvwaqegpDZiW"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttIgFBH83f85"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hceo-Uii3f85",
        "outputId": "7d7ff620-2a35-4200-ac15-a686bd772cd5"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-l7w32n3f86"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "RN1JgCTz9Upe"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f820ba9-dc5b-4141-85c2-c38397c16963",
        "id": "Dr3XtNL8Az96"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "42/42 [==============================] - 9s 168ms/step - loss: 1.0730 - acc: 0.4411 - val_loss: 1.0187 - val_acc: 0.4637\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 4s 91ms/step - loss: 0.9297 - acc: 0.5801 - val_loss: 0.9402 - val_acc: 0.5821\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 4s 88ms/step - loss: 0.8415 - acc: 0.6435 - val_loss: 0.8983 - val_acc: 0.6047\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 4s 88ms/step - loss: 0.7826 - acc: 0.6586 - val_loss: 0.8918 - val_acc: 0.6166\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 4s 88ms/step - loss: 0.7244 - acc: 0.6858 - val_loss: 0.8967 - val_acc: 0.6314\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 4s 87ms/step - loss: 0.6837 - acc: 0.7009 - val_loss: 0.8524 - val_acc: 0.6519\n",
            "Epoch 7/10\n",
            "42/42 [==============================] - 4s 86ms/step - loss: 0.6640 - acc: 0.7221 - val_loss: 0.8506 - val_acc: 0.6378\n",
            "Epoch 8/10\n",
            "42/42 [==============================] - 4s 88ms/step - loss: 0.6017 - acc: 0.7432 - val_loss: 0.8455 - val_acc: 0.6321\n",
            "Epoch 9/10\n",
            "42/42 [==============================] - 4s 88ms/step - loss: 0.5922 - acc: 0.7613 - val_loss: 0.8569 - val_acc: 0.6526\n",
            "Epoch 10/10\n",
            "42/42 [==============================] - 4s 89ms/step - loss: 0.5834 - acc: 0.7795 - val_loss: 0.8890 - val_acc: 0.6399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best.hdf5')"
      ],
      "metadata": {
        "id": "ut5tB8-NC5yX"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSqZ50AJC5yY"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "pwJVCm6BFVFi"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20 %"
      ],
      "metadata": {
        "id": "ivoGBJPyNgdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.2"
      ],
      "metadata": {
        "id": "aKI5muahNgdo"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JokRosdTNgdo"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb_j8ZOfNgdo",
        "outputId": "321be7e2-9773-4b7a-abf6-4d27505ef4d4"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfVaIgUANgdp"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "tf-mjDAjNgdp"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oClDbEUwNgdp",
        "outputId": "28501e24-0abd-45c6-eedc-84a219ee6176"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "83/83 [==============================] - 6s 71ms/step - loss: 0.4604 - acc: 0.8308 - val_loss: 0.6089 - val_acc: 0.7639\n",
            "Epoch 2/10\n",
            "83/83 [==============================] - 6s 67ms/step - loss: 0.4123 - acc: 0.8384 - val_loss: 0.5695 - val_acc: 0.7872\n",
            "Epoch 3/10\n",
            "83/83 [==============================] - 7s 85ms/step - loss: 0.3978 - acc: 0.8233 - val_loss: 0.5960 - val_acc: 0.7879\n",
            "Epoch 4/10\n",
            "83/83 [==============================] - 9s 105ms/step - loss: 0.3672 - acc: 0.8565 - val_loss: 0.5938 - val_acc: 0.7815\n",
            "Epoch 5/10\n",
            "83/83 [==============================] - 10s 123ms/step - loss: 0.3485 - acc: 0.8852 - val_loss: 0.7080 - val_acc: 0.7308\n",
            "Epoch 6/10\n",
            "83/83 [==============================] - 9s 110ms/step - loss: 0.3002 - acc: 0.8852 - val_loss: 0.6449 - val_acc: 0.7731\n",
            "Epoch 7/10\n",
            "83/83 [==============================] - 9s 110ms/step - loss: 0.2994 - acc: 0.8852 - val_loss: 0.6277 - val_acc: 0.7773\n",
            "Epoch 8/10\n",
            "83/83 [==============================] - 10s 120ms/step - loss: 0.2802 - acc: 0.8973 - val_loss: 0.6838 - val_acc: 0.7646\n",
            "Epoch 9/10\n",
            "83/83 [==============================] - 9s 106ms/step - loss: 0.2725 - acc: 0.9003 - val_loss: 0.6770 - val_acc: 0.7696\n",
            "Epoch 10/10\n",
            "83/83 [==============================] - 9s 114ms/step - loss: 0.2296 - acc: 0.9305 - val_loss: 0.6847 - val_acc: 0.7674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_2.hdf5')"
      ],
      "metadata": {
        "id": "qCCRqstnNgdp"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcv6ouAgNgdp"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "DUvsEYoUNgdp"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30 %"
      ],
      "metadata": {
        "id": "ZwSbUoCdNjIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.3"
      ],
      "metadata": {
        "id": "fUCiuE-6NjIk"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU8bO4BcNjIk"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZGCsm_CNjIk",
        "outputId": "63114d95-6fbf-435e-cf03-204f90175fbc"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkmkKWSKNjIk"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "pDmtqCflNjIk"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvuOrU6GNjIk",
        "outputId": "e2046af8-0ef9-440f-936f-b038446bce25"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_3.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 11s 67ms/step - loss: 0.9976 - acc: 0.5196 - val_loss: 0.9871 - val_acc: 0.5236\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 0.8363 - acc: 0.6485 - val_loss: 0.9052 - val_acc: 0.6180\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 0.7687 - acc: 0.6677 - val_loss: 0.7631 - val_acc: 0.6779\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 0.6972 - acc: 0.7080 - val_loss: 0.7633 - val_acc: 0.7026\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 0.6566 - acc: 0.7311 - val_loss: 0.7505 - val_acc: 0.6808\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 0.5950 - acc: 0.7694 - val_loss: 0.6462 - val_acc: 0.7322\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 8s 64ms/step - loss: 0.5479 - acc: 0.7724 - val_loss: 0.6379 - val_acc: 0.7357\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 8s 63ms/step - loss: 0.5241 - acc: 0.8046 - val_loss: 0.6028 - val_acc: 0.7632\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 8s 61ms/step - loss: 0.5109 - acc: 0.7966 - val_loss: 0.6662 - val_acc: 0.7491\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 0.4613 - acc: 0.8197 - val_loss: 0.5816 - val_acc: 0.7815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_3.hdf5')"
      ],
      "metadata": {
        "id": "XPrtVtn7NjIl"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6TN_OBFNjIl"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "eLkLuchENjIl"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40 %"
      ],
      "metadata": {
        "id": "vHItovu8NkJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.4"
      ],
      "metadata": {
        "id": "7Pj62n2bNkJk"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYZyOYLUNkJk"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAxU79jNNkJk",
        "outputId": "c92c1aa0-cb6a-4a39-9817-6d3dd0bb423f"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0GM6I41NkJk"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "gnLGGF5xNkJk"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xbGS2c0NkJl",
        "outputId": "5065a74f-25d8-4d12-aa65-2cc5a0632418"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_4.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "166/166 [==============================] - 12s 60ms/step - loss: 0.9715 - acc: 0.5249 - val_loss: 0.8669 - val_acc: 0.6209\n",
            "Epoch 2/10\n",
            "166/166 [==============================] - 10s 58ms/step - loss: 0.8308 - acc: 0.6276 - val_loss: 0.8654 - val_acc: 0.6286\n",
            "Epoch 3/10\n",
            "166/166 [==============================] - 10s 59ms/step - loss: 0.7391 - acc: 0.6737 - val_loss: 0.6708 - val_acc: 0.7315\n",
            "Epoch 4/10\n",
            "166/166 [==============================] - 9s 57ms/step - loss: 0.6603 - acc: 0.7228 - val_loss: 0.6611 - val_acc: 0.7407\n",
            "Epoch 5/10\n",
            "166/166 [==============================] - 10s 58ms/step - loss: 0.5799 - acc: 0.7621 - val_loss: 0.6580 - val_acc: 0.7449\n",
            "Epoch 6/10\n",
            "166/166 [==============================] - 10s 59ms/step - loss: 0.5394 - acc: 0.7908 - val_loss: 0.5475 - val_acc: 0.7949\n",
            "Epoch 7/10\n",
            "166/166 [==============================] - 10s 58ms/step - loss: 0.4890 - acc: 0.7938 - val_loss: 0.5458 - val_acc: 0.7914\n",
            "Epoch 8/10\n",
            "166/166 [==============================] - 10s 60ms/step - loss: 0.4649 - acc: 0.8255 - val_loss: 0.5734 - val_acc: 0.7780\n",
            "Epoch 9/10\n",
            "166/166 [==============================] - 10s 58ms/step - loss: 0.4378 - acc: 0.8369 - val_loss: 0.5616 - val_acc: 0.7935\n",
            "Epoch 10/10\n",
            "166/166 [==============================] - 10s 59ms/step - loss: 0.4351 - acc: 0.8399 - val_loss: 0.5423 - val_acc: 0.8034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_4.hdf5')"
      ],
      "metadata": {
        "id": "aWq83TQHNkJl"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvmbaMF9NkJl"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "qLXONCmnNkJl"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 %"
      ],
      "metadata": {
        "id": "1jiz628NNlFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.5"
      ],
      "metadata": {
        "id": "sR7odUhQNlFC"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr54sWOdNlFC"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CRgyRP3NlFC",
        "outputId": "33ca19e1-f207-46b0-e83e-54e332f0adf3"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW1RXG_SNlFC"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "e1HaGAdNNlFD"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLVivMgNNlFD",
        "outputId": "2eab31e6-5258-4b90-da69-e31c751486b2"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_5.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "207/207 [==============================] - 15s 62ms/step - loss: 0.9787 - acc: 0.5329 - val_loss: 0.8628 - val_acc: 0.5955\n",
            "Epoch 2/10\n",
            "207/207 [==============================] - 13s 62ms/step - loss: 0.8140 - acc: 0.6417 - val_loss: 0.7131 - val_acc: 0.6998\n",
            "Epoch 3/10\n",
            "207/207 [==============================] - 13s 62ms/step - loss: 0.6948 - acc: 0.6991 - val_loss: 0.6604 - val_acc: 0.7357\n",
            "Epoch 4/10\n",
            "207/207 [==============================] - 13s 61ms/step - loss: 0.6228 - acc: 0.7468 - val_loss: 0.6257 - val_acc: 0.7625\n",
            "Epoch 5/10\n",
            "207/207 [==============================] - 13s 62ms/step - loss: 0.5539 - acc: 0.7843 - val_loss: 0.5721 - val_acc: 0.7865\n",
            "Epoch 6/10\n",
            "207/207 [==============================] - 13s 62ms/step - loss: 0.5139 - acc: 0.8042 - val_loss: 0.6358 - val_acc: 0.7498\n",
            "Epoch 7/10\n",
            "207/207 [==============================] - 13s 61ms/step - loss: 0.4874 - acc: 0.8030 - val_loss: 0.5592 - val_acc: 0.7914\n",
            "Epoch 8/10\n",
            "207/207 [==============================] - 13s 61ms/step - loss: 0.4708 - acc: 0.8169 - val_loss: 0.5553 - val_acc: 0.7935\n",
            "Epoch 9/10\n",
            "207/207 [==============================] - 13s 61ms/step - loss: 0.4396 - acc: 0.8296 - val_loss: 0.5584 - val_acc: 0.7977\n",
            "Epoch 10/10\n",
            "207/207 [==============================] - 13s 63ms/step - loss: 0.4317 - acc: 0.8350 - val_loss: 0.5915 - val_acc: 0.7907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_5.hdf5')"
      ],
      "metadata": {
        "id": "8rmFKhCpNlFD"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwiDBVw7NlFD"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "v7qiM6Z8NlFD"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 60 %"
      ],
      "metadata": {
        "id": "WISlpiYSNmEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.6"
      ],
      "metadata": {
        "id": "6auqx7kUNmEJ"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T5posEFNmEJ"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXtz8uC3NmEJ",
        "outputId": "3ba5b245-df27-4cd7-b971-1e0dca75752d"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzr1kHz3NmEJ"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "ds1rgeEyNmEJ"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCp_JwdDNmEK",
        "outputId": "3e13e9cc-f8f1-4686-ed64-b3990543522f"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_6.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "249/249 [==============================] - 16s 57ms/step - loss: 0.9588 - acc: 0.5448 - val_loss: 0.8431 - val_acc: 0.6385\n",
            "Epoch 2/10\n",
            "249/249 [==============================] - 14s 56ms/step - loss: 0.8006 - acc: 0.6571 - val_loss: 0.6689 - val_acc: 0.7188\n",
            "Epoch 3/10\n",
            "249/249 [==============================] - 14s 58ms/step - loss: 0.6787 - acc: 0.7155 - val_loss: 0.5581 - val_acc: 0.7872\n",
            "Epoch 4/10\n",
            "249/249 [==============================] - 14s 58ms/step - loss: 0.5948 - acc: 0.7523 - val_loss: 0.5080 - val_acc: 0.8147\n",
            "Epoch 5/10\n",
            "249/249 [==============================] - 14s 56ms/step - loss: 0.5373 - acc: 0.7880 - val_loss: 0.5423 - val_acc: 0.7886\n",
            "Epoch 6/10\n",
            "249/249 [==============================] - 14s 56ms/step - loss: 0.5023 - acc: 0.8041 - val_loss: 0.5379 - val_acc: 0.7956\n",
            "Epoch 7/10\n",
            "249/249 [==============================] - 14s 55ms/step - loss: 0.4884 - acc: 0.8097 - val_loss: 0.4909 - val_acc: 0.8203\n",
            "Epoch 8/10\n",
            "249/249 [==============================] - 14s 55ms/step - loss: 0.4634 - acc: 0.8207 - val_loss: 0.5391 - val_acc: 0.7970\n",
            "Epoch 9/10\n",
            "249/249 [==============================] - 14s 55ms/step - loss: 0.4458 - acc: 0.8293 - val_loss: 0.4953 - val_acc: 0.8154\n",
            "Epoch 10/10\n",
            "249/249 [==============================] - 14s 55ms/step - loss: 0.4377 - acc: 0.8318 - val_loss: 0.4884 - val_acc: 0.8062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_6.hdf5')"
      ],
      "metadata": {
        "id": "ggdtU5ACNmEK"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eavd_CzVNmEK"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "1hJ4-tZKNmEK"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 70 %"
      ],
      "metadata": {
        "id": "MjOnEc_yNm-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.7"
      ],
      "metadata": {
        "id": "kX_g-BlYNm-w"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8PfiHc1Nm-w"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTPxDxcRNm-x",
        "outputId": "3024799d-6dab-4d3d-c17f-d19e7977424d"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lME1dg04Nm-x"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "pNUYv6kXNm-x"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGRvfaJ6Nm-x",
        "outputId": "f2d05e4c-f143-41d7-df12-73c074663f1e"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_7.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "290/290 [==============================] - 19s 58ms/step - loss: 0.9311 - acc: 0.5662 - val_loss: 0.8312 - val_acc: 0.6321\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 16s 57ms/step - loss: 0.7457 - acc: 0.6858 - val_loss: 0.7528 - val_acc: 0.7040\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 17s 58ms/step - loss: 0.6043 - acc: 0.7648 - val_loss: 0.5917 - val_acc: 0.7717\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 17s 58ms/step - loss: 0.5463 - acc: 0.7872 - val_loss: 0.5132 - val_acc: 0.8125\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 17s 60ms/step - loss: 0.5174 - acc: 0.8041 - val_loss: 0.5236 - val_acc: 0.8132\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 18s 62ms/step - loss: 0.4916 - acc: 0.8123 - val_loss: 0.4827 - val_acc: 0.8238\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 16s 57ms/step - loss: 0.4560 - acc: 0.8325 - val_loss: 0.4954 - val_acc: 0.8189\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 16s 56ms/step - loss: 0.4389 - acc: 0.8312 - val_loss: 0.4988 - val_acc: 0.8175\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 16s 57ms/step - loss: 0.4075 - acc: 0.8455 - val_loss: 0.5776 - val_acc: 0.7956\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 16s 55ms/step - loss: 0.3961 - acc: 0.8494 - val_loss: 0.5293 - val_acc: 0.8168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_7.hdf5')"
      ],
      "metadata": {
        "id": "6vrIYa3XNm-x"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfwtYRyQNm-x"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "oBrqIXS4Nm-x"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 80 %"
      ],
      "metadata": {
        "id": "38GLoTuNNoE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.8"
      ],
      "metadata": {
        "id": "8d7yA-XzNoE_"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaJ6BnRNNoE_"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTfgR6RKNoE_",
        "outputId": "b30b853d-5cb2-4fec-f4c5-6023eb21906d"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj8huppHNoE_"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "XLuaFctVNoFA"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzcywH-JNoFA",
        "outputId": "c2088853-0a52-4cc7-82f8-dc80a44bbe1a"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_8.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "331/331 [==============================] - 14s 37ms/step - loss: 0.9264 - acc: 0.5631 - val_loss: 0.7392 - val_acc: 0.6885\n",
            "Epoch 2/10\n",
            "331/331 [==============================] - 12s 36ms/step - loss: 0.7202 - acc: 0.6975 - val_loss: 0.6079 - val_acc: 0.7526\n",
            "Epoch 3/10\n",
            "331/331 [==============================] - 12s 35ms/step - loss: 0.5897 - acc: 0.7677 - val_loss: 0.5079 - val_acc: 0.8048\n",
            "Epoch 4/10\n",
            "331/331 [==============================] - 12s 35ms/step - loss: 0.5275 - acc: 0.8021 - val_loss: 0.5042 - val_acc: 0.8104\n",
            "Epoch 5/10\n",
            "331/331 [==============================] - 12s 35ms/step - loss: 0.4893 - acc: 0.8161 - val_loss: 0.4946 - val_acc: 0.8083\n",
            "Epoch 6/10\n",
            "331/331 [==============================] - 12s 35ms/step - loss: 0.4673 - acc: 0.8225 - val_loss: 0.4950 - val_acc: 0.8090\n",
            "Epoch 7/10\n",
            "331/331 [==============================] - 11s 35ms/step - loss: 0.4441 - acc: 0.8259 - val_loss: 0.5026 - val_acc: 0.8168\n",
            "Epoch 8/10\n",
            "331/331 [==============================] - 12s 36ms/step - loss: 0.4350 - acc: 0.8327 - val_loss: 0.4928 - val_acc: 0.8182\n",
            "Epoch 9/10\n",
            "331/331 [==============================] - 12s 35ms/step - loss: 0.4097 - acc: 0.8497 - val_loss: 0.5043 - val_acc: 0.8132\n",
            "Epoch 10/10\n",
            "331/331 [==============================] - 11s 35ms/step - loss: 0.3957 - acc: 0.8516 - val_loss: 0.4974 - val_acc: 0.8147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_8.hdf5')"
      ],
      "metadata": {
        "id": "_AsrDLGsNoFA"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZENqj28NoFA"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "LMtMoojsNoFA"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 90 %"
      ],
      "metadata": {
        "id": "LGRtnR6KN5pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 0.9"
      ],
      "metadata": {
        "id": "wOJ21EQyN5pr"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKEolxe9N5pr"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KGcPOY3N5pr",
        "outputId": "458f8da6-d395-4aa7-f125-2a73cc049a6d"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QpuLAKcN5pr"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "Yrg4NIPcN5pr"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I22q8TuwN5pr",
        "outputId": "0bf6cf35-8d8c-4baf-fbda-f537bccf8ced"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_9.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "373/373 [==============================] - 23s 55ms/step - loss: 0.9090 - acc: 0.5707 - val_loss: 0.7956 - val_acc: 0.6681\n",
            "Epoch 2/10\n",
            "373/373 [==============================] - 20s 53ms/step - loss: 0.6868 - acc: 0.7120 - val_loss: 0.5992 - val_acc: 0.7752\n",
            "Epoch 3/10\n",
            "373/373 [==============================] - 20s 53ms/step - loss: 0.5717 - acc: 0.7788 - val_loss: 0.5235 - val_acc: 0.8147\n",
            "Epoch 4/10\n",
            "373/373 [==============================] - 20s 53ms/step - loss: 0.5292 - acc: 0.7882 - val_loss: 0.4975 - val_acc: 0.8217\n",
            "Epoch 5/10\n",
            "373/373 [==============================] - 20s 54ms/step - loss: 0.5064 - acc: 0.8066 - val_loss: 0.4887 - val_acc: 0.8365\n",
            "Epoch 6/10\n",
            "373/373 [==============================] - 20s 53ms/step - loss: 0.4812 - acc: 0.8150 - val_loss: 0.5261 - val_acc: 0.8048\n",
            "Epoch 7/10\n",
            "373/373 [==============================] - 20s 54ms/step - loss: 0.4695 - acc: 0.8171 - val_loss: 0.4936 - val_acc: 0.8407\n",
            "Epoch 8/10\n",
            "373/373 [==============================] - 19s 52ms/step - loss: 0.4340 - acc: 0.8288 - val_loss: 0.5024 - val_acc: 0.8372\n",
            "Epoch 9/10\n",
            "373/373 [==============================] - 20s 54ms/step - loss: 0.4224 - acc: 0.8322 - val_loss: 0.5020 - val_acc: 0.8414\n",
            "Epoch 10/10\n",
            "373/373 [==============================] - 21s 57ms/step - loss: 0.4015 - acc: 0.8426 - val_loss: 0.5174 - val_acc: 0.8273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_9.hdf5')"
      ],
      "metadata": {
        "id": "OpXz1fDKN5pr"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDs31WDkN5pr"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "88smunjkN5pr"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 100 %"
      ],
      "metadata": {
        "id": "ExyJ53M8N60f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 1"
      ],
      "metadata": {
        "id": "x3GWVzQvN60f"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXc_Z_crN60g"
      },
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUK9jPMYN60g",
        "outputId": "cdc98c44-afa2-4ac2-bbff-1dc0edf3d523"
      },
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bxq2F6EN60g"
      },
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d= 60\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(GRU(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "NT3Vh7S8N60g"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VRY8GgXN60g",
        "outputId": "36d7fb92-9e07-4c91-a99d-27c0234df8e9"
      },
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_100.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "414/414 [==============================] - 26s 57ms/step - loss: 0.8764 - acc: 0.5950 - val_loss: 1.0286 - val_acc: 0.6152\n",
            "Epoch 2/10\n",
            "414/414 [==============================] - 22s 53ms/step - loss: 0.6521 - acc: 0.7363 - val_loss: 0.5993 - val_acc: 0.7646\n",
            "Epoch 3/10\n",
            "414/414 [==============================] - 22s 53ms/step - loss: 0.5592 - acc: 0.7735 - val_loss: 0.5168 - val_acc: 0.8013\n",
            "Epoch 4/10\n",
            "414/414 [==============================] - 22s 53ms/step - loss: 0.5136 - acc: 0.8043 - val_loss: 0.5133 - val_acc: 0.8013\n",
            "Epoch 5/10\n",
            "414/414 [==============================] - 22s 53ms/step - loss: 0.4812 - acc: 0.8173 - val_loss: 0.4987 - val_acc: 0.8196\n",
            "Epoch 6/10\n",
            "414/414 [==============================] - 22s 52ms/step - loss: 0.4654 - acc: 0.8206 - val_loss: 0.4946 - val_acc: 0.8189\n",
            "Epoch 7/10\n",
            "414/414 [==============================] - 22s 52ms/step - loss: 0.4506 - acc: 0.8248 - val_loss: 0.4915 - val_acc: 0.8224\n",
            "Epoch 8/10\n",
            "414/414 [==============================] - 22s 53ms/step - loss: 0.4295 - acc: 0.8333 - val_loss: 0.5111 - val_acc: 0.8175\n",
            "Epoch 9/10\n",
            "414/414 [==============================] - 22s 52ms/step - loss: 0.4122 - acc: 0.8466 - val_loss: 0.4846 - val_acc: 0.8266\n",
            "Epoch 10/10\n",
            "414/414 [==============================] - 21s 52ms/step - loss: 0.3910 - acc: 0.8529 - val_loss: 0.5090 - val_acc: 0.8132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_100.hdf5')"
      ],
      "metadata": {
        "id": "8xKEgfa6N60g"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eckgd-QN60g"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_GRU.append(accu)"
      ],
      "metadata": {
        "id": "9Pj19JvUN60g"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save history"
      ],
      "metadata": {
        "id": "LL4RgymoOsTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('GRU_history.pkl', 'wb') as f:\n",
        "  pickle.dump(history_GRU, f)"
      ],
      "metadata": {
        "id": "xSfwW8kqOvRB"
      },
      "execution_count": 213,
      "outputs": []
    }
  ]
}