{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_O4sGPh8JUR"
      },
      "source": [
        "This jupyter nodebook is for RQ2 of the project to measure what is the minimum required data of SO dataset to achieve acceptable results compared to maximum data size. I start from 10% of the total amount of data to 100%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbgo8IbD8KpU"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSsLMO_FBhiP",
        "outputId": "d0c7d8db-d1c8-4683-d836-10d80272b250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBr5eUCxB_PZ",
        "outputId": "f3e0ebf5-4b98-4733-f97b-bd441600d87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/SE project/dataset/EMTK_datasets-v1.0.zip\n",
            "c200f78878252a3779c76f19be2f0230c38ef00b\n",
            "   creating: collab-uniba-EMTK_datasets-c200f78/\n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/LICENSE  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/README.md  \n",
            "   creating: collab-uniba-EMTK_datasets-c200f78/jira/\n",
            "   creating: collab-uniba-EMTK_datasets-c200f78/jira/emotions/\n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/jira/emotions/README.md  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/jira/emotions/anger.csv  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/jira/emotions/joy.csv  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/jira/emotions/love.csv  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/jira/emotions/sadness.csv  \n",
            "   creating: collab-uniba-EMTK_datasets-c200f78/so/\n",
            "   creating: collab-uniba-EMTK_datasets-c200f78/so/emotions/\n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/so/emotions/Emotion Annotation Guidelines.pdf  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/so/emotions/Emotions_GoldSandard_andAnnotation.xlsx  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/so/emotions/README.md  \n",
            "   creating: collab-uniba-EMTK_datasets-c200f78/so/polarity/\n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/so/polarity/README.md  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/so/polarity/test1326itemPOLARITY.csv  \n",
            "  inflating: collab-uniba-EMTK_datasets-c200f78/so/polarity/train3098itemPOLARITY.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/SE\\ project/dataset/EMTK_datasets-v1.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ArWqQX1MCX2S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('/content/collab-uniba-EMTK_datasets-c200f78/so/emotions/Emotions_GoldSandard_andAnnotation.xlsx',sheet_name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1mMC9KHDvBI",
        "outputId": "d6dfba3e-c9d9-4283-c209-2240f6432547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Anger_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Fear_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Joy_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        JOY        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Love_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...       LOVE        NaN        NaN\n",
              " 1        A  Second           2  ...       LOVE        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...       LOVE        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...       LOVE        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Sadness_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...    SADNESS        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns],\n",
              " 'Surprise_all':      Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              " 0        A  Second           1  ...        NaN        NaN        NaN\n",
              " 1        A  Second           2  ...        NaN        NaN        NaN\n",
              " 2        A  Second           3  ...        NaN        NaN        NaN\n",
              " 3        A  Second           4  ...        NaN        NaN        NaN\n",
              " 4        A  Second           5  ...        NaN        NaN        NaN\n",
              " ...    ...     ...         ...  ...        ...        ...        ...\n",
              " 4795     D   Third         496  ...        NaN        NaN        NaN\n",
              " 4796     D   Third         497  ...        NaN        NaN        NaN\n",
              " 4797     D   Third         498  ...        NaN        NaN        NaN\n",
              " 4798     D   Third         499  ...        NaN        NaN        NaN\n",
              " 4799     D   Third         500  ...        NaN        NaN        NaN\n",
              " \n",
              " [4800 rows x 10 columns]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mioJoc2uDMTH",
        "outputId": "3e33ef8b-bb92-4cbc-c208-3d5319d70a2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-97cab531-9647-44ae-abf4-e940510b606c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Group</th>\n",
              "      <th>Set</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Text</th>\n",
              "      <th>rater 1</th>\n",
              "      <th>rater 2</th>\n",
              "      <th>rater 3</th>\n",
              "      <th>Gold Label</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>1</td>\n",
              "      <td>SVG transform on text attribute works excellen...</td>\n",
              "      <td>X</td>\n",
              "      <td>NaN</td>\n",
              "      <td>X</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>2</td>\n",
              "      <td>Excellent! This is exactly what I needed. Thanks!</td>\n",
              "      <td>X</td>\n",
              "      <td>x</td>\n",
              "      <td>X</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>3</td>\n",
              "      <td>Have added a modern solution as of May 2014 in...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>4</td>\n",
              "      <td>Have you tried removing 'preload' attribute? (...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>Second</td>\n",
              "      <td>5</td>\n",
              "      <td>A smarter, entirely C++-way of doing what you ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>496</td>\n",
              "      <td>Yes - that feature is extremely useful for wri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>497</td>\n",
              "      <td>Works great! And you can add \"desc\" after the ...</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>498</td>\n",
              "      <td>Yeah, I didn't know about the non-greedy thing...</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>499</td>\n",
              "      <td>Fortunately I'm doing *very* little with Offic...</td>\n",
              "      <td>x</td>\n",
              "      <td>X</td>\n",
              "      <td>x</td>\n",
              "      <td>LOVE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>D</td>\n",
              "      <td>Third</td>\n",
              "      <td>500</td>\n",
              "      <td>Another very fast approach is the [seek method...</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97cab531-9647-44ae-abf4-e940510b606c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97cab531-9647-44ae-abf4-e940510b606c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97cab531-9647-44ae-abf4-e940510b606c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Group     Set  Unnamed: 2  ... Gold Label Unnamed: 8 Unnamed: 9\n",
              "0        A  Second           1  ...       LOVE        NaN        NaN\n",
              "1        A  Second           2  ...       LOVE        NaN        NaN\n",
              "2        A  Second           3  ...        NaN        NaN        NaN\n",
              "3        A  Second           4  ...        NaN        NaN        NaN\n",
              "4        A  Second           5  ...        NaN        NaN        NaN\n",
              "...    ...     ...         ...  ...        ...        ...        ...\n",
              "4795     D   Third         496  ...        NaN        NaN        NaN\n",
              "4796     D   Third         497  ...       LOVE        NaN        NaN\n",
              "4797     D   Third         498  ...        NaN        NaN        NaN\n",
              "4798     D   Third         499  ...       LOVE        NaN        NaN\n",
              "4799     D   Third         500  ...        NaN        NaN        NaN\n",
              "\n",
              "[4800 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df[\"Love_all\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kxq2ahs_D55P"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "neutral_emotional = []\n",
        "label_sets = []\n",
        "\n",
        "for i in range(4800):\n",
        "  \n",
        "  label = 0\n",
        "  label_set = []\n",
        "  for sheet in ['Love_all', 'Joy_all', 'Anger_all', 'Sadness_all', 'Fear_all', 'Surprise_all']:\n",
        "    gold_label = df[sheet].loc[i].iat[7]\n",
        "    if gold_label in ['LOVE', 'JOY', 'ANGER', 'SADNESS', 'FEAR', 'SURPRISE']:\n",
        "      label = 1\n",
        "      label_set.append(gold_label)\n",
        "  if not 'SURPRISE' in label_set:\n",
        "    neutral_emotional.append(label)\n",
        "    label_sets.append(label_set)\n",
        "    text.append(df['Love_all'].loc[i].iat[3])\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IiSCS3Mxg0zA"
      },
      "outputs": [],
      "source": [
        "def not_both(label_set,i):\n",
        "  positive = ['LOVE','JOY']\n",
        "  negative = ['SADNESS', 'FEAR', 'ANGER']\n",
        "  is_positive = False\n",
        "  is_negative = False\n",
        "  for label in label_set:\n",
        "    if label in positive:\n",
        "      is_positive = True\n",
        "    elif label in negative:\n",
        "      is_negative = True\n",
        "  if is_negative and is_positive:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def find_semantic(label_set):\n",
        "  positive = ['LOVE','JOY']\n",
        "  negative = ['SADNESS', 'FEAR', 'ANGER']\n",
        "  if label_set[0] in positive:\n",
        "    return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sVfodv6Dcm8W"
      },
      "outputs": [],
      "source": [
        "text_semantic = []\n",
        "semantic = []\n",
        "\n",
        "for i in range(4755):\n",
        "  if neutral_emotional[i]:\n",
        "    if not_both(label_sets[i],i):\n",
        "      text_semantic.append(text[i])\n",
        "      semantic.append(find_semantic(label_sets[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dcUWip-WlGFt"
      },
      "outputs": [],
      "source": [
        "text_all = []\n",
        "three_cat = []\n",
        "\n",
        "for i in range(4755):\n",
        "  if neutral_emotional[i]==0:\n",
        "    text_all.append(text[i])\n",
        "    three_cat.append('Neutral')\n",
        "for i in range(len(text_semantic)):\n",
        "  text_all.append(text_semantic[i])\n",
        "  if semantic[i]:\n",
        "    three_cat.append('Positive')\n",
        "  else:\n",
        "    three_cat.append('Negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muMmkfO74RTr",
        "outputId": "1e479406-3aa5-4eed-cdd2-637e4c9def06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4730"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(text_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThoTEPk7xoK3"
      },
      "source": [
        "# Model setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wvPs_TW45uhC"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyGXUB1d4iTS",
        "outputId": "7d1b93fd-1a3c-45d7-a86c-01275312d25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Neutral' 'Neutral' 'Neutral' ... 'Positive' 'Positive' 'Positive']\n",
            "[1 1 1 ... 2 2 2]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# define example\n",
        "data = three_cat\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3eLp3pM3fHL",
        "outputId": "adad8d8c-7b11-4810-8e17-e69276f5dda9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3311.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(text_all)*0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5JsxpVD_FpWJ"
      },
      "outputs": [],
      "source": [
        "history_LSTM  = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VOchltPC3pV"
      },
      "source": [
        "# 10 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvwaqegpDZiW"
      },
      "outputs": [],
      "source": [
        "rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ttIgFBH83f85"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hceo-Uii3f85",
        "outputId": "91a62e77-d388-420c-fbfe-e0b6b388c86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k-l7w32n3f86"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RN1JgCTz9Upe"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr3XtNL8Az96",
        "outputId": "f5776b8e-fbad-4712-d515-a41283188e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "42/42 [==============================] - 9s 74ms/step - loss: 1.0660 - acc: 0.4804 - val_loss: 1.0336 - val_acc: 0.4489\n",
            "Epoch 2/15\n",
            "42/42 [==============================] - 2s 60ms/step - loss: 0.9734 - acc: 0.5438 - val_loss: 0.9220 - val_acc: 0.6075\n",
            "Epoch 3/15\n",
            "42/42 [==============================] - 3s 61ms/step - loss: 0.8705 - acc: 0.6556 - val_loss: 0.8513 - val_acc: 0.6385\n",
            "Epoch 4/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.7772 - acc: 0.6526 - val_loss: 0.9527 - val_acc: 0.5384\n",
            "Epoch 5/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.7252 - acc: 0.6828 - val_loss: 0.7902 - val_acc: 0.6540\n",
            "Epoch 6/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.6429 - acc: 0.7341 - val_loss: 0.7643 - val_acc: 0.6822\n",
            "Epoch 7/15\n",
            "42/42 [==============================] - 4s 85ms/step - loss: 0.6427 - acc: 0.7432 - val_loss: 0.7453 - val_acc: 0.6906\n",
            "Epoch 8/15\n",
            "42/42 [==============================] - 2s 60ms/step - loss: 0.5940 - acc: 0.7795 - val_loss: 0.7384 - val_acc: 0.7054\n",
            "Epoch 9/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.5446 - acc: 0.7764 - val_loss: 0.7771 - val_acc: 0.6864\n",
            "Epoch 10/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.4811 - acc: 0.8066 - val_loss: 0.7734 - val_acc: 0.6892\n",
            "Epoch 11/15\n",
            "42/42 [==============================] - 2s 58ms/step - loss: 0.4796 - acc: 0.8218 - val_loss: 0.7719 - val_acc: 0.6885\n",
            "Epoch 12/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.4146 - acc: 0.8520 - val_loss: 0.7899 - val_acc: 0.6977\n",
            "Epoch 13/15\n",
            "42/42 [==============================] - 4s 86ms/step - loss: 0.3837 - acc: 0.8550 - val_loss: 0.8447 - val_acc: 0.6716\n",
            "Epoch 14/15\n",
            "42/42 [==============================] - 2s 59ms/step - loss: 0.3676 - acc: 0.8520 - val_loss: 0.7848 - val_acc: 0.6871\n",
            "Epoch 15/15\n",
            "42/42 [==============================] - 2s 58ms/step - loss: 0.3023 - acc: 0.9184 - val_loss: 0.9137 - val_acc: 0.6885\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_1.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ut5tB8-NC5yX"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_1.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YSqZ50AJC5yY"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pwJVCm6BFVFi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivoGBJPyNgdk"
      },
      "source": [
        "# 20 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aKI5muahNgdo"
      },
      "outputs": [],
      "source": [
        "rate = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JokRosdTNgdo"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb_j8ZOfNgdo",
        "outputId": "bc9bacb5-bf8c-4a0c-99cd-25467d0fb207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nfVaIgUANgdp"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tf-mjDAjNgdp"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oClDbEUwNgdp",
        "outputId": "020311d8-cf74-4304-b6a5-579808eddb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "83/83 [==============================] - 7s 61ms/step - loss: 0.9925 - acc: 0.5136 - val_loss: 1.0158 - val_acc: 0.5159\n",
            "Epoch 2/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.8413 - acc: 0.6344 - val_loss: 0.8213 - val_acc: 0.6342\n",
            "Epoch 3/15\n",
            "83/83 [==============================] - 4s 54ms/step - loss: 0.7573 - acc: 0.6873 - val_loss: 0.7894 - val_acc: 0.6638\n",
            "Epoch 4/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.6988 - acc: 0.7205 - val_loss: 0.7484 - val_acc: 0.6681\n",
            "Epoch 5/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.6225 - acc: 0.7613 - val_loss: 0.8297 - val_acc: 0.6610\n",
            "Epoch 6/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.6006 - acc: 0.7613 - val_loss: 0.9859 - val_acc: 0.5934\n",
            "Epoch 7/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.5812 - acc: 0.7764 - val_loss: 0.7014 - val_acc: 0.7005\n",
            "Epoch 8/15\n",
            "83/83 [==============================] - 4s 54ms/step - loss: 0.5310 - acc: 0.8021 - val_loss: 0.8107 - val_acc: 0.6610\n",
            "Epoch 9/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.5079 - acc: 0.8157 - val_loss: 0.7450 - val_acc: 0.6942\n",
            "Epoch 10/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.4642 - acc: 0.8202 - val_loss: 0.7013 - val_acc: 0.7174\n",
            "Epoch 11/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.4413 - acc: 0.8248 - val_loss: 0.7737 - val_acc: 0.7104\n",
            "Epoch 12/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.4007 - acc: 0.8384 - val_loss: 0.8158 - val_acc: 0.6885\n",
            "Epoch 13/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.3589 - acc: 0.8716 - val_loss: 0.8315 - val_acc: 0.6885\n",
            "Epoch 14/15\n",
            "83/83 [==============================] - 3s 41ms/step - loss: 0.3717 - acc: 0.8625 - val_loss: 0.6942 - val_acc: 0.7364\n",
            "Epoch 15/15\n",
            "83/83 [==============================] - 4s 54ms/step - loss: 0.3250 - acc: 0.8867 - val_loss: 0.8762 - val_acc: 0.6864\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qCCRqstnNgdp"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_2.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Rcv6ouAgNgdp"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DUvsEYoUNgdp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwSbUoCdNjIj"
      },
      "source": [
        "# 30 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fUCiuE-6NjIk"
      },
      "outputs": [],
      "source": [
        "rate = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zU8bO4BcNjIk"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZGCsm_CNjIk",
        "outputId": "185aa9cd-6129-4482-b60b-f999ffcb95c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HkmkKWSKNjIk"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pDmtqCflNjIk"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvuOrU6GNjIk",
        "outputId": "ccf6b147-8402-4622-f957-dcc1a8a7f4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "125/125 [==============================] - 9s 51ms/step - loss: 0.9738 - acc: 0.5438 - val_loss: 1.0231 - val_acc: 0.5292\n",
            "Epoch 2/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.7983 - acc: 0.6506 - val_loss: 1.0310 - val_acc: 0.5574\n",
            "Epoch 3/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.7336 - acc: 0.6757 - val_loss: 0.7094 - val_acc: 0.6991\n",
            "Epoch 4/15\n",
            "125/125 [==============================] - 5s 44ms/step - loss: 0.6659 - acc: 0.7311 - val_loss: 0.6659 - val_acc: 0.7174\n",
            "Epoch 5/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.6315 - acc: 0.7362 - val_loss: 0.7718 - val_acc: 0.6582\n",
            "Epoch 6/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.5895 - acc: 0.7644 - val_loss: 0.6085 - val_acc: 0.7590\n",
            "Epoch 7/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.5650 - acc: 0.7754 - val_loss: 0.5836 - val_acc: 0.7653\n",
            "Epoch 8/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.5180 - acc: 0.7925 - val_loss: 0.6856 - val_acc: 0.7216\n",
            "Epoch 9/15\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.5008 - acc: 0.7986 - val_loss: 0.6001 - val_acc: 0.7759\n",
            "Epoch 10/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.4614 - acc: 0.8087 - val_loss: 0.6923 - val_acc: 0.7245\n",
            "Epoch 11/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.4245 - acc: 0.8278 - val_loss: 0.7104 - val_acc: 0.7407\n",
            "Epoch 12/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.4077 - acc: 0.8308 - val_loss: 0.6308 - val_acc: 0.7541\n",
            "Epoch 13/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.3814 - acc: 0.8469 - val_loss: 0.7019 - val_acc: 0.7435\n",
            "Epoch 14/15\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.3441 - acc: 0.8751 - val_loss: 0.6210 - val_acc: 0.7717\n",
            "Epoch 15/15\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.3287 - acc: 0.8671 - val_loss: 0.6249 - val_acc: 0.7773\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_3.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "XPrtVtn7NjIl"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_3.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "i6TN_OBFNjIl"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eLkLuchENjIl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHItovu8NkJk"
      },
      "source": [
        "# 40 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7Pj62n2bNkJk"
      },
      "outputs": [],
      "source": [
        "rate = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jYZyOYLUNkJk"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAxU79jNNkJk",
        "outputId": "1bc84e02-ca72-43f1-ca76-9905e6d499bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "w0GM6I41NkJk"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "gnLGGF5xNkJk"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xbGS2c0NkJl",
        "outputId": "e84588b5-122b-4854-e691-1cdc3a256a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "166/166 [==============================] - 9s 42ms/step - loss: 0.9364 - acc: 0.5718 - val_loss: 0.7744 - val_acc: 0.6709\n",
            "Epoch 2/15\n",
            "166/166 [==============================] - 5s 31ms/step - loss: 0.7902 - acc: 0.6480 - val_loss: 0.6938 - val_acc: 0.7019\n",
            "Epoch 3/15\n",
            "166/166 [==============================] - 5s 32ms/step - loss: 0.7210 - acc: 0.7024 - val_loss: 0.6437 - val_acc: 0.7329\n",
            "Epoch 4/15\n",
            "166/166 [==============================] - 6s 38ms/step - loss: 0.6476 - acc: 0.7379 - val_loss: 1.0423 - val_acc: 0.6547\n",
            "Epoch 5/15\n",
            "166/166 [==============================] - 5s 32ms/step - loss: 0.5877 - acc: 0.7628 - val_loss: 0.6923 - val_acc: 0.7259\n",
            "Epoch 6/15\n",
            "166/166 [==============================] - 6s 38ms/step - loss: 0.5518 - acc: 0.7817 - val_loss: 0.5810 - val_acc: 0.7766\n",
            "Epoch 7/15\n",
            "166/166 [==============================] - 6s 38ms/step - loss: 0.5039 - acc: 0.7998 - val_loss: 0.5726 - val_acc: 0.7646\n",
            "Epoch 8/15\n",
            "166/166 [==============================] - 5s 32ms/step - loss: 0.4807 - acc: 0.8127 - val_loss: 0.5605 - val_acc: 0.7858\n",
            "Epoch 9/15\n",
            "166/166 [==============================] - 5s 32ms/step - loss: 0.4471 - acc: 0.8308 - val_loss: 0.5680 - val_acc: 0.7879\n",
            "Epoch 10/15\n",
            "166/166 [==============================] - 5s 32ms/step - loss: 0.4273 - acc: 0.8399 - val_loss: 0.5459 - val_acc: 0.7935\n",
            "Epoch 11/15\n",
            "166/166 [==============================] - 6s 38ms/step - loss: 0.3881 - acc: 0.8542 - val_loss: 0.5726 - val_acc: 0.7893\n",
            "Epoch 12/15\n",
            "166/166 [==============================] - 5s 32ms/step - loss: 0.3800 - acc: 0.8520 - val_loss: 0.5719 - val_acc: 0.7879\n",
            "Epoch 13/15\n",
            "166/166 [==============================] - 5s 31ms/step - loss: 0.3414 - acc: 0.8708 - val_loss: 0.6073 - val_acc: 0.8013\n",
            "Epoch 14/15\n",
            "166/166 [==============================] - 5s 31ms/step - loss: 0.3096 - acc: 0.8784 - val_loss: 0.6627 - val_acc: 0.7703\n",
            "Epoch 15/15\n",
            "166/166 [==============================] - 5s 31ms/step - loss: 0.2988 - acc: 0.8837 - val_loss: 0.7641 - val_acc: 0.7421\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_4.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aWq83TQHNkJl"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_4.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "SvmbaMF9NkJl"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qLXONCmnNkJl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jiz628NNlFC"
      },
      "source": [
        "# 50 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "sR7odUhQNlFC"
      },
      "outputs": [],
      "source": [
        "rate = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dr54sWOdNlFC"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CRgyRP3NlFC",
        "outputId": "b34e9d27-086d-4d22-e9e5-b31586d23dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YW1RXG_SNlFC"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "e1HaGAdNNlFD"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLVivMgNNlFD",
        "outputId": "5154b24d-9f60-4f24-e468-04b09bb033c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "207/207 [==============================] - 9s 33ms/step - loss: 0.9326 - acc: 0.5801 - val_loss: 0.8050 - val_acc: 0.6321\n",
            "Epoch 2/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.7582 - acc: 0.6767 - val_loss: 0.7448 - val_acc: 0.6815\n",
            "Epoch 3/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.7005 - acc: 0.7172 - val_loss: 0.6706 - val_acc: 0.7097\n",
            "Epoch 4/15\n",
            "207/207 [==============================] - 7s 35ms/step - loss: 0.6284 - acc: 0.7444 - val_loss: 0.6818 - val_acc: 0.6977\n",
            "Epoch 5/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.5703 - acc: 0.7668 - val_loss: 0.6594 - val_acc: 0.7555\n",
            "Epoch 6/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.5394 - acc: 0.7861 - val_loss: 0.5843 - val_acc: 0.7689\n",
            "Epoch 7/15\n",
            "207/207 [==============================] - 7s 35ms/step - loss: 0.5060 - acc: 0.7903 - val_loss: 0.5765 - val_acc: 0.7583\n",
            "Epoch 8/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.4556 - acc: 0.8278 - val_loss: 0.5784 - val_acc: 0.7858\n",
            "Epoch 9/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.4502 - acc: 0.8193 - val_loss: 0.5696 - val_acc: 0.7956\n",
            "Epoch 10/15\n",
            "207/207 [==============================] - 7s 35ms/step - loss: 0.4146 - acc: 0.8411 - val_loss: 0.5556 - val_acc: 0.7935\n",
            "Epoch 11/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.4000 - acc: 0.8502 - val_loss: 0.6078 - val_acc: 0.7780\n",
            "Epoch 12/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.3658 - acc: 0.8556 - val_loss: 0.5541 - val_acc: 0.7858\n",
            "Epoch 13/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.3436 - acc: 0.8767 - val_loss: 0.5942 - val_acc: 0.7900\n",
            "Epoch 14/15\n",
            "207/207 [==============================] - 6s 30ms/step - loss: 0.3100 - acc: 0.8767 - val_loss: 0.5926 - val_acc: 0.7970\n",
            "Epoch 15/15\n",
            "207/207 [==============================] - 6s 29ms/step - loss: 0.3000 - acc: 0.8822 - val_loss: 0.6035 - val_acc: 0.7815\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_5.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8rmFKhCpNlFD"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_5.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "kwiDBVw7NlFD"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "v7qiM6Z8NlFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WISlpiYSNmEJ"
      },
      "source": [
        "# 60 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "6auqx7kUNmEJ"
      },
      "outputs": [],
      "source": [
        "rate = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0T5posEFNmEJ"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "sXtz8uC3NmEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c29997-8189-4fbf-a199-70f08b903e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Gzr1kHz3NmEJ"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ds1rgeEyNmEJ"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "KCp_JwdDNmEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29eb80aa-2e93-4504-bce3-b4af6afd5944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "249/249 [==============================] - 10s 31ms/step - loss: 0.9076 - acc: 0.5947 - val_loss: 0.8486 - val_acc: 0.6096\n",
            "Epoch 2/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.7446 - acc: 0.6883 - val_loss: 0.9144 - val_acc: 0.6441\n",
            "Epoch 3/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.6748 - acc: 0.7231 - val_loss: 0.6581 - val_acc: 0.7294\n",
            "Epoch 4/15\n",
            "249/249 [==============================] - 8s 33ms/step - loss: 0.5892 - acc: 0.7608 - val_loss: 0.6590 - val_acc: 0.7174\n",
            "Epoch 5/15\n",
            "249/249 [==============================] - 8s 33ms/step - loss: 0.5380 - acc: 0.7875 - val_loss: 0.6341 - val_acc: 0.7442\n",
            "Epoch 6/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.5030 - acc: 0.8051 - val_loss: 0.5610 - val_acc: 0.7815\n",
            "Epoch 7/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.4831 - acc: 0.8082 - val_loss: 0.6856 - val_acc: 0.7414\n",
            "Epoch 8/15\n",
            "249/249 [==============================] - 8s 33ms/step - loss: 0.4522 - acc: 0.8207 - val_loss: 0.5472 - val_acc: 0.8006\n",
            "Epoch 9/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.4169 - acc: 0.8338 - val_loss: 0.5862 - val_acc: 0.7900\n",
            "Epoch 10/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.4077 - acc: 0.8439 - val_loss: 0.5589 - val_acc: 0.7949\n",
            "Epoch 11/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.3753 - acc: 0.8570 - val_loss: 0.8454 - val_acc: 0.7245\n",
            "Epoch 12/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.3464 - acc: 0.8706 - val_loss: 0.5952 - val_acc: 0.7928\n",
            "Epoch 13/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.3363 - acc: 0.8776 - val_loss: 0.6382 - val_acc: 0.7829\n",
            "Epoch 14/15\n",
            "249/249 [==============================] - 7s 28ms/step - loss: 0.3173 - acc: 0.8751 - val_loss: 0.6317 - val_acc: 0.7801\n",
            "Epoch 15/15\n",
            "249/249 [==============================] - 8s 33ms/step - loss: 0.2933 - acc: 0.8867 - val_loss: 0.6854 - val_acc: 0.7879\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_6.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ggdtU5ACNmEK"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_6.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "eavd_CzVNmEK"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "1hJ4-tZKNmEK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjOnEc_yNm-w"
      },
      "source": [
        "# 70 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "kX_g-BlYNm-w"
      },
      "outputs": [],
      "source": [
        "rate = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "X8PfiHc1Nm-w"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTPxDxcRNm-x",
        "outputId": "ec7be1c2-3c54-4a37-a99a-2d348c785e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lME1dg04Nm-x"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pNUYv6kXNm-x"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGRvfaJ6Nm-x",
        "outputId": "7aab6e2a-8ab5-4cf6-a055-136e94b7d2c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "290/290 [==============================] - 11s 29ms/step - loss: 0.9100 - acc: 0.5874 - val_loss: 0.7469 - val_acc: 0.6744\n",
            "Epoch 2/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.7476 - acc: 0.6867 - val_loss: 0.7583 - val_acc: 0.7082\n",
            "Epoch 3/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.6742 - acc: 0.7315 - val_loss: 0.6208 - val_acc: 0.7463\n",
            "Epoch 4/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.5875 - acc: 0.7678 - val_loss: 0.5505 - val_acc: 0.7829\n",
            "Epoch 5/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.5491 - acc: 0.7902 - val_loss: 0.5369 - val_acc: 0.7984\n",
            "Epoch 6/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.5098 - acc: 0.8041 - val_loss: 0.5333 - val_acc: 0.7844\n",
            "Epoch 7/15\n",
            "290/290 [==============================] - 9s 31ms/step - loss: 0.4810 - acc: 0.8144 - val_loss: 0.5596 - val_acc: 0.7674\n",
            "Epoch 8/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.4533 - acc: 0.8192 - val_loss: 0.5040 - val_acc: 0.8055\n",
            "Epoch 9/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.4221 - acc: 0.8377 - val_loss: 0.5104 - val_acc: 0.8217\n",
            "Epoch 10/15\n",
            "290/290 [==============================] - 9s 31ms/step - loss: 0.3899 - acc: 0.8558 - val_loss: 0.5321 - val_acc: 0.8217\n",
            "Epoch 11/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.3919 - acc: 0.8468 - val_loss: 0.5114 - val_acc: 0.8175\n",
            "Epoch 12/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.3642 - acc: 0.8567 - val_loss: 0.5430 - val_acc: 0.8140\n",
            "Epoch 13/15\n",
            "290/290 [==============================] - 8s 27ms/step - loss: 0.3238 - acc: 0.8774 - val_loss: 0.5892 - val_acc: 0.7956\n",
            "Epoch 14/15\n",
            "290/290 [==============================] - 9s 31ms/step - loss: 0.3191 - acc: 0.8874 - val_loss: 0.6149 - val_acc: 0.7914\n",
            "Epoch 15/15\n",
            "290/290 [==============================] - 8s 28ms/step - loss: 0.2963 - acc: 0.8869 - val_loss: 0.5789 - val_acc: 0.8034\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_7.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6vrIYa3XNm-x"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_7.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "JfwtYRyQNm-x"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "oBrqIXS4Nm-x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GLoTuNNoE_"
      },
      "source": [
        "# 80 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8d7yA-XzNoE_"
      },
      "outputs": [],
      "source": [
        "rate = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "uaJ6BnRNNoE_"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTfgR6RKNoE_",
        "outputId": "cd838ae1-5fc2-45bc-9aec-a6b990aaccc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "sj8huppHNoE_"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "XLuaFctVNoFA"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzcywH-JNoFA",
        "outputId": "4b99147f-77a5-41df-e59f-e1dcdf387283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "331/331 [==============================] - 12s 31ms/step - loss: 0.8886 - acc: 0.5940 - val_loss: 0.8257 - val_acc: 0.6392\n",
            "Epoch 2/15\n",
            "331/331 [==============================] - 11s 33ms/step - loss: 0.7128 - acc: 0.7002 - val_loss: 0.7150 - val_acc: 0.7223\n",
            "Epoch 3/15\n",
            "331/331 [==============================] - 10s 30ms/step - loss: 0.6212 - acc: 0.7455 - val_loss: 0.5907 - val_acc: 0.7583\n",
            "Epoch 4/15\n",
            "331/331 [==============================] - 11s 33ms/step - loss: 0.5586 - acc: 0.7764 - val_loss: 0.5297 - val_acc: 0.7942\n",
            "Epoch 5/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.5210 - acc: 0.8032 - val_loss: 0.5321 - val_acc: 0.7970\n",
            "Epoch 6/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.4930 - acc: 0.8089 - val_loss: 0.5255 - val_acc: 0.8013\n",
            "Epoch 7/15\n",
            "331/331 [==============================] - 11s 33ms/step - loss: 0.4698 - acc: 0.8214 - val_loss: 0.5239 - val_acc: 0.8069\n",
            "Epoch 8/15\n",
            "331/331 [==============================] - 10s 30ms/step - loss: 0.4292 - acc: 0.8365 - val_loss: 0.5153 - val_acc: 0.8069\n",
            "Epoch 9/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.4209 - acc: 0.8384 - val_loss: 0.5421 - val_acc: 0.8076\n",
            "Epoch 10/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.3960 - acc: 0.8523 - val_loss: 0.5199 - val_acc: 0.8013\n",
            "Epoch 11/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.3676 - acc: 0.8591 - val_loss: 0.6064 - val_acc: 0.7900\n",
            "Epoch 12/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.3485 - acc: 0.8674 - val_loss: 0.5253 - val_acc: 0.8069\n",
            "Epoch 13/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.3288 - acc: 0.8788 - val_loss: 0.5534 - val_acc: 0.8041\n",
            "Epoch 14/15\n",
            "331/331 [==============================] - 11s 33ms/step - loss: 0.2922 - acc: 0.8875 - val_loss: 0.6076 - val_acc: 0.8062\n",
            "Epoch 15/15\n",
            "331/331 [==============================] - 10s 29ms/step - loss: 0.2934 - acc: 0.8924 - val_loss: 0.5513 - val_acc: 0.8125\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_8.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "_AsrDLGsNoFA"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_8.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "XZENqj28NoFA"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "LMtMoojsNoFA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGRtnR6KN5pn"
      },
      "source": [
        "# 90 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "wOJ21EQyN5pr"
      },
      "outputs": [],
      "source": [
        "rate = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "fKEolxe9N5pr"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KGcPOY3N5pr",
        "outputId": "45a64405-eb05-4f13-c649-7dfd2b606aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "1QpuLAKcN5pr"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Yrg4NIPcN5pr"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I22q8TuwN5pr",
        "outputId": "f31ca09a-3f90-45e8-f3d7-79d38870ee79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "373/373 [==============================] - 15s 32ms/step - loss: 0.8721 - acc: 0.6052 - val_loss: 0.7351 - val_acc: 0.6836\n",
            "Epoch 2/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.7058 - acc: 0.7036 - val_loss: 0.6378 - val_acc: 0.7259\n",
            "Epoch 3/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.6124 - acc: 0.7486 - val_loss: 0.5992 - val_acc: 0.7576\n",
            "Epoch 4/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.5583 - acc: 0.7707 - val_loss: 0.5704 - val_acc: 0.7745\n",
            "Epoch 5/15\n",
            "373/373 [==============================] - 11s 29ms/step - loss: 0.5232 - acc: 0.7949 - val_loss: 0.6245 - val_acc: 0.7773\n",
            "Epoch 6/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.4737 - acc: 0.8184 - val_loss: 0.5695 - val_acc: 0.7935\n",
            "Epoch 7/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.4612 - acc: 0.8228 - val_loss: 0.5056 - val_acc: 0.8259\n",
            "Epoch 8/15\n",
            "373/373 [==============================] - 11s 29ms/step - loss: 0.4333 - acc: 0.8322 - val_loss: 0.5235 - val_acc: 0.8118\n",
            "Epoch 9/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.4153 - acc: 0.8412 - val_loss: 0.5307 - val_acc: 0.8083\n",
            "Epoch 10/15\n",
            "373/373 [==============================] - 11s 30ms/step - loss: 0.3986 - acc: 0.8419 - val_loss: 0.5343 - val_acc: 0.8111\n",
            "Epoch 11/15\n",
            "373/373 [==============================] - 11s 29ms/step - loss: 0.3745 - acc: 0.8543 - val_loss: 0.5547 - val_acc: 0.8175\n",
            "Epoch 12/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.3527 - acc: 0.8560 - val_loss: 0.5374 - val_acc: 0.8013\n",
            "Epoch 13/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.3268 - acc: 0.8728 - val_loss: 0.5811 - val_acc: 0.8034\n",
            "Epoch 14/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.3059 - acc: 0.8812 - val_loss: 0.5647 - val_acc: 0.8076\n",
            "Epoch 15/15\n",
            "373/373 [==============================] - 10s 26ms/step - loss: 0.2754 - acc: 0.8966 - val_loss: 0.6018 - val_acc: 0.8034\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_9.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "OpXz1fDKN5pr"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_9.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "XDs31WDkN5pr"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "88smunjkN5pr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExyJ53M8N60f"
      },
      "source": [
        "# 100 %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "x3GWVzQvN60f"
      },
      "outputs": [],
      "source": [
        "rate = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "IXc_Z_crN60g"
      },
      "outputs": [],
      "source": [
        "maxlen = 100 # cut point in the review\n",
        "training_samples = int(len(text_all)*0.7*rate) # train size\n",
        "validation_samples = len(text_all)-int(len(text_all)*0.7) # Validates size\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_all)\n",
        "sequences = tokenizer.texts_to_sequences(text_all)\n",
        "word_index = tokenizer.word_index               \n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(onehot_encoded)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "X_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "X_test = data[training_samples:training_samples+validation_samples] \n",
        "y_test = labels[training_samples:training_samples+validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUK9jPMYN60g",
        "outputId": "b635b91c-0de0-45f0-a3ad-03ef258df7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# word embedding subsection\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding='utf-8') #added , encoding='utf-8'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"found %s word vectors.\" % len (embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "8Bxq2F6EN60g"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # GloVe contains 100-dimensional embedding vectors for 400.000 words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 100)\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "NT3Vh7S8N60g"
      },
      "outputs": [],
      "source": [
        "d= 80\n",
        "\n",
        "# built the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(LSTM(d, dropout = 0.3))\n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# compile\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "            loss = \"categorical_crossentropy\",\n",
        "            metrics = [\"acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "3VRY8GgXN60g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58334c41-d1ef-4a07-cf69-f59cf3d6c4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "414/414 [==============================] - 14s 27ms/step - loss: 0.8592 - acc: 0.6249 - val_loss: 0.6912 - val_acc: 0.7153\n",
            "Epoch 2/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.7041 - acc: 0.7101 - val_loss: 0.6617 - val_acc: 0.7160\n",
            "Epoch 3/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.6025 - acc: 0.7496 - val_loss: 0.5463 - val_acc: 0.7963\n",
            "Epoch 4/15\n",
            "414/414 [==============================] - 12s 29ms/step - loss: 0.5521 - acc: 0.7795 - val_loss: 0.6138 - val_acc: 0.7773\n",
            "Epoch 5/15\n",
            "414/414 [==============================] - 12s 30ms/step - loss: 0.5083 - acc: 0.7989 - val_loss: 0.5334 - val_acc: 0.7963\n",
            "Epoch 6/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.4704 - acc: 0.8188 - val_loss: 0.5252 - val_acc: 0.7956\n",
            "Epoch 7/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.4564 - acc: 0.8288 - val_loss: 0.4886 - val_acc: 0.8273\n",
            "Epoch 8/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.4232 - acc: 0.8372 - val_loss: 0.5256 - val_acc: 0.8231\n",
            "Epoch 9/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.4041 - acc: 0.8445 - val_loss: 0.5433 - val_acc: 0.7942\n",
            "Epoch 10/15\n",
            "414/414 [==============================] - 11s 25ms/step - loss: 0.3844 - acc: 0.8505 - val_loss: 0.4836 - val_acc: 0.8273\n",
            "Epoch 11/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.3615 - acc: 0.8641 - val_loss: 0.5112 - val_acc: 0.8161\n",
            "Epoch 12/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.3342 - acc: 0.8777 - val_loss: 0.5199 - val_acc: 0.8224\n",
            "Epoch 13/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.3262 - acc: 0.8735 - val_loss: 0.5459 - val_acc: 0.8083\n",
            "Epoch 14/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.3021 - acc: 0.8831 - val_loss: 0.5611 - val_acc: 0.8238\n",
            "Epoch 15/15\n",
            "414/414 [==============================] - 11s 26ms/step - loss: 0.2897 - acc: 0.8873 - val_loss: 0.5495 - val_acc: 0.8111\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "mcp_save = ModelCheckpoint('model_best_100.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_data=(X_test,y_test), callbacks=[mcp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "8xKEgfa6N60g"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_best_100.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "4eckgd-QN60g"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test_1 = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "9Pj19JvUN60g"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accu = classification_report(y_test_1, y_pred, output_dict = True)['accuracy']\n",
        "history_LSTM.append(accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL4RgymoOsTB"
      },
      "source": [
        "# save history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "xSfwW8kqOvRB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('LSTM_history.pkl', 'wb') as f:\n",
        "  pickle.dump(history_LSTM, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_LSTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYHGHkPrfquI",
        "outputId": "6cd1aae5-bf60-478f-92d9-bc77d2197052"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7054263565891473,\n",
              " 0.7364341085271318,\n",
              " 0.7653276955602537,\n",
              " 0.7935165609584214,\n",
              " 0.7857646229739254,\n",
              " 0.8005637773079634,\n",
              " 0.8054968287526427,\n",
              " 0.806906272022551,\n",
              " 0.8259337561663143,\n",
              " 0.8273431994362227]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Training_size_test_for_LSTM_based_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}